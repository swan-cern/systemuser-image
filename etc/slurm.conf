# /etc/slurm/slurm.conf generated by puppet
#
# Configuration file containing common parameters for headnode and workernode.
#

# SLURMCTLD HOSTS
ControlMachine=slurm03.cern.ch
ControlAddr=slurm03.cern.ch
BackupController=slurm04.cern.ch
BackupAddr=slurm04.cern.ch


# GENERAL OPTIONS
AllowSpecResourcesUsage=0
#ChosLoc=
CoreSpecPlugin=core_spec/none
CpuFreqDef=Performance
CpuFreqGovernors=OnDemand,Performance
DisableRootJobs=NO
EnforcePartLimits=YES
ExtSensorsType=ext_sensors/none
ExtSensorsFreq=0
FirstJobId=1
MaxJobId=999999
GresTypes=Hepspec,Hepspec_per_core
GroupUpdateForce=0
JobContainerType=job_container/none
JobFileAppend=0
JobRequeue=0
JobSubmitPlugins=require_timelimit
KillOnBadExit=1
LaunchType=launch/slurm
#LaunchParameters=
#InteractiveStepOptions=
#Licenses=
#NodeFeaturesPlugins=
MailProg=/bin/mail
#MailDomain=
MaxJobCount=10000
MaxStepCount=40000
PluginDir=/usr/lib64/slurm
#PlugStackConfig=
PowerPlugin=power/none
#PowerParameters=
PreemptType=preempt/partition_prio
PreemptMode=CANCEL
#PrivateData=
ProctrackType=proctrack/cgroup
PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
#ReconfigFlags=
#ResvEpilog=
#ResvProlog=
ReturnToService=2
#BcastExclude=/lib,/usr/lib,/lib64,/usr/lib64
#SbcastParameters=
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPlugstack=
SlurmctldParameters=idle_on_node_suspend,cloud_dns,enable_configless
#CommunicationParameters=
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPlugstack=
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
SlurmdUser=root
#SrunEpilog=
#SrunProlog=
#SrunPortRange=
StateSaveLocation=/hpcscratch/statesavelocation/batch-testing
SwitchType=switch/none
TaskPlugin=task/cgroup,task/affinity
#TaskPluginParam=
#TaskEpilog=
TaskProlog=/etc/slurm/taskprolog.sh
TCPTimeout=2
TmpFS=/tmp
TrackWCKey=no
UnkillableStepProgram=/etc/slurm/job_stuck_alert.sh


# SECURITY
AuthType=auth/munge
#AuthInfo=
CredType=cred/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
MCSPlugin=mcs/none
#MCSParameters=
UsePAM=0
AuthAltTypes=auth/jwt


# TIMERS
BatchStartTimeout=10
CompleteWait=0
EioTimeout=60
EpilogMsgTime=2000
GetEnvTimeout=2
GroupUpdateTime=600
InactiveLimit=0
#KeepAliveTime=
KillWait=30
MessageTimeout=10
MinJobAge=300
OverTimeLimit=0
PrologEpilogTimeout=0
ResvOverRun=0
SlurmctldTimeout=120
SlurmdTimeout=300
UnkillableStepTimeout=480
Waittime=0


# SCHEDULING
# DefMemPerCPU=4000
#DefMemPerNode=
#Epilog=
#EpilogSlurmctld=
MaxArraySize=1001
MaxMemPerCPU=0
MaxMemPerNode=0
MaxTasksPerNode=512
MpiDefault=pmi2
#MpiParams=
#PrologSlurmctld=
#Prolog=
PrologFlags=X11,Contain
#X11Parameters=
#RequeueExit=
#RequeueExitHold=
SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerParameters=bf_max_job_test=500,default_queue_depth=500,bf_window=43200
SelectType=select/cons_res
SelectTypeParameters=CR_CPU_Memory
VSizeFactor=0


# JOB PRIORITY
PriorityType=priority/multifactor
PriorityFlags=SMALL_RELATIVE_TO_TIME
PriorityCalcPeriod=5
PriorityDecayHalfLife=7-0
PriorityFavorSmall=NO
PriorityMaxAge=7-0
PriorityUsageResetPeriod=NONE
PriorityWeightAge=2500
PriorityWeightFairshare=2500
FairShareDampeningFactor=1
PriorityWeightJobSize=2500
PriorityWeightPartition=0
PriorityWeightQOS=0
#PriorityWeightTRES=


# ACCOUNTING
ClusterName=batch-testing
#DefaultStorageHost=
#DefaultStorageType=
#DefaultStorageUser=
#DefaultStoragePass=
#DefaultStorageLoc=
AccountingStorageHost=slurmdb03.cern.ch
AccountingStorageBackupHost=slurmdb04.cern.ch
AccountingStoragePort=6819
#AccountingStorageEnforce=
#AccountingStorageTRES=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
#AccountingStoreFlags=
#AccountingStoragePass=
#AccountingStorageLoc=
JobCompType=jobcomp/none
#JobCompHost=
JobCompPort=6819
#JobCompUser=
#JobCompPass=
#JobCompLoc=
JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherParams=
JobAcctGatherFrequency=task=30,energy=0,network=0,filesystem=0
AcctGatherNodeFreq=0
AcctGatherEnergyType=acct_gather_energy/none
AcctGatherInterconnectType=acct_gather_interconnect/none
AcctGatherFilesystemType=acct_gather_filesystem/none
AcctGatherProfileType=acct_gather_profile/none


# LOGGING
#DebugFlags=
LogTimeFormat=iso8601_ms
SlurmctldDebug=info
SlurmctldSyslogDebug=info
#SlurmctldLogFile=
SlurmdDebug=info
SlurmdSyslogDebug=info
#SlurmdLogFile=
SlurmSchedLogLevel=0
#SlurmSchedLogFile=


# HEALTH CHECK
HealthCheckProgram=/usr/sbin/nhc
HealthCheckNodeState=ANY
HealthCheckInterval=600


# POWER SAVE SUPPORT FOR IDLE NODES
SuspendProgram=/home/slurm/suspend
SuspendTimeout=100
SuspendRate=60
SuspendTime=2200
#SuspendExcNodes=
SuspendExcParts=batch-short,photest
ResumeProgram=/home/slurm/resume
ResumeTimeout=2000
ResumeRate=20
ResumeFailProgram=/home/slurm/resumefail


# TOPOLOGY
TopologyPlugin=topology/tree
#TopologyParam=
RoutePlugin=route/topology
TreeWidth=2


# COMPUTE NODES
NodeName=hpc-cloud-test[001-004] CPUs=2 CoresPerSocket=1 Sockets=1 ThreadsPerCore=2 RealMemory=3300 TmpDisk=500 State=FUTURE Gres=Hepspec:351,Hepspec_per_core:1096
NodeName=slurmgate[05,06] State=DRAIN Reason="Submit node only"
NodeName=hpc-be[037,101,143-144] CPUs=40 CoresPerSocket=10 Sockets=2 ThreadsPerCore=2 RealMemory=128000 TmpDisk=5000 State=FUTURE Gres=Hepspec:388,Hepspec_per_core:97
NodeName=hpc-photon[069-072] CPUs=32 CoresPerSocket=16 ThreadsPerCore=1 RealMemory=512000 TmpDisk=10000 State=FUTURE Gres=Hepspec:32,Hepspec_per_core:1


# PARTITIONS
PartitionName=batch-short Nodes=hpc-cloud-test[001-004] Default=NO DefMemPerCPU=1000 MaxMemPerCPU=1000 DefaultTime=2-0 MaxTime=2-0 ExclusiveUser=YES State=UP
PartitionName=photest Nodes=hpc-photon[069-072] Default=NO DefMemPerCPU=16000 MaxMemPerCPU=16000 DefaultTime=21-0 MaxTime=21-0 ExclusiveUser=YES State=UP
